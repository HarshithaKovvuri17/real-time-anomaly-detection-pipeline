# ğŸš¨ğŸ“Š Real-Time Anomaly Detection Pipeline
=====================================================

ğŸ“Œ PROJECT OVERVIEW
------------------
This project implements a complete, production-style
**REAL-TIME DATA STREAMING PIPELINE** to detect anomalies
in continuously generated event data.

The system simulates live device metrics ğŸ“¡, processes them
using **Apache Kafka**, performs stateful anomaly detection,
stores detected anomalies in **PostgreSQL**, and visualizes
them using a **real-time dashboard** ğŸ“ˆ.

This project reflects real-world use cases such as:
- ğŸ’³ Fraud detection
- ğŸŒ IoT device monitoring
- âš™ï¸ Operational intelligence
- ğŸ“Š Streaming analytics systems


ğŸ§  KEY CONCEPTS COVERED
----------------------
- ğŸ”„ Event-driven architecture
- â±ï¸ Real-time data streaming
- ğŸ“¨ Apache Kafka producers and consumers
- ğŸ§® Stateful stream processing
- ğŸ“ Rolling window statistics
- ğŸ³ Dockerized microservices
- ğŸ“Š Real-time dashboards
- ğŸ›¡ï¸ Fault-tolerant data pipelines


ğŸ—ï¸ ARCHITECTURE
---------------
```

Python Producer ğŸ
   â†“
Kafka Topic (raw_events) ğŸ“¨
   â†“
Kafka Consumer (Anomaly Detection) ğŸš¨
   â†“
Kafka Topic (anomalies) ğŸ“Œ
   â†“
PostgreSQL (anomalies table) ğŸ—„ï¸
   â†“
Streamlit Dashboard (Real-Time View) ğŸ“ˆ
```

ğŸ§© SYSTEM COMPONENTS
-------------------
1ï¸âƒ£ Producer
   - Generates continuous synthetic events
   - Publishes events to Kafka topic: raw_events

2ï¸âƒ£ Kafka
   - Acts as the central message broker
   - Decouples producers and consumers
   - Ensures durability and scalability

3ï¸âƒ£ Consumer (Anomaly Detector)
   - Consumes events from raw_events
   - Maintains rolling windows per device
   - Detects anomalies using statistical rules
   - Publishes anomalies to anomalies topic

4ï¸âƒ£ PostgreSQL
   - Persists detected anomalies
   - Acts as the system of record

5ï¸âƒ£ Dashboard
   - Reads data from PostgreSQL
   - Displays anomalies in near real-time
   - Auto-refreshes every few seconds â³


ğŸš¨ ANOMALY DETECTION LOGIC
-------------------------
Each device is analyzed **INDEPENDENTLY** using a rolling
statistical window ğŸ“Š.

Processing steps:
1. ğŸ“¥ Read incoming event
2. ğŸ†” Group events by device_id
3. ğŸªŸ Maintain rolling window of last N metrics
4. ğŸ“ Compute mean and standard deviation
5. âš ï¸ Apply anomaly detection rule
6. ğŸš¨ Publish anomaly if detected
7. ğŸ’¾ Persist anomaly to PostgreSQL


ğŸ“ DETECTION RULE
----------------
An event is classified as an anomaly if:

|metric_value âˆ’ mean| > threshold Ã— standard_deviation

Where:
- metric_value : current incoming metric
- mean         : mean of rolling window
- std_dev      : standard deviation of rolling window
- threshold    : configurable (default = 3)
- window size  : configurable (default = 100)

ğŸ“ Detection is performed **PER DEVICE**.


âš™ï¸ TECH STACK
-------------
- ğŸ“¨ Apache Kafka
- ğŸ§­ Zookeeper
- ğŸ Python
- ğŸ—„ï¸ PostgreSQL
- ğŸ“Š Streamlit
- ğŸ³ Docker
- ğŸ“¦ Docker Compose


ğŸ—‚ï¸ PROJECT STRUCTURE
--------------------
```

real-time-anomaly-detection-pipeline/
â”‚
â”œâ”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ producer/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ consumer/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ postgres/
â”‚   â””â”€â”€ init.sql
â”‚
â””â”€â”€ README.md
```
---

ğŸš€ SETUP & EXECUTION
-------------------
Prerequisites:
- ğŸ³ Docker Desktop
- ğŸ“¦ Docker Compose
- ğŸŒ± Git
- ğŸ Python 3.9+

Steps:
1. Clone the repository
2. Navigate to project directory
3. Run: docker-compose up --build
4. Wait for all services to start â³

---

ğŸ“Š VIEWING OUTPUTS
-----------------
Dashboard ğŸ“ˆ:
- URL: http://localhost:8501
- Displays detected anomalies
- Auto-refreshes every few seconds

Consumer Logs ğŸ–¥ï¸:
- docker logs -f consumer
- Shows anomaly detection messages in real time

PostgreSQL Validation ğŸ—„ï¸:
- docker exec -it postgres psql -U anomaly_user -d anomalies_db
- Query anomalies table to verify persistence

---

ğŸ” BEST PRACTICES IMPLEMENTED
-----------------------------
- ğŸ”§ Environment variables for configuration
- âŒ No hardcoded credentials
- ğŸ”— Decoupled services
- â™»ï¸ Stateless producers and consumers
- ğŸ›‘ Graceful shutdown handling
- ğŸ›¡ï¸ Fault-tolerant message processing

---

âœ… VERIFIED OUTCOMES
------------------
âœ” Continuous event streaming  
âœ” Real-time anomaly detection  
âœ” Kafka topic decoupling  
âœ” PostgreSQL persistence  
âœ” Live dashboard visualization  
âœ” Stable long-running execution  

---

ğŸ“š KEY LEARNINGS
---------------
- Designing streaming architectures
- Working with Kafka producers & consumers
- Implementing stateful logic in streaming systems
- Container orchestration using Docker Compose
- Debugging distributed systems
- End-to-end observability of data pipelines

---

ğŸš€ FUTURE ENHANCEMENTS
---------------------
- ğŸ¤– Machine learning-based anomaly detection
- ğŸ“© Alerting via Email / Slack
- ğŸ“ˆ Kafka topic partition scaling
- ğŸ“Š Monitoring (Prometheus + Grafana)
- ğŸ” Authentication & security
- â˜ï¸ Cloud deployment (AWS / GCP / Azure)

---

ğŸ FINAL CONCLUSION
------------------
This project represents a complete, real-world,
production-style **REAL-TIME ANOMALY DETECTION PIPELINE** ğŸš¨.

It is suitable for:
- ğŸ“ Data engineering portfolios
- ğŸ’¼ Technical interviews
- ğŸ“ Hands-on learning
- ğŸ§  Demonstrating streaming system expertise

